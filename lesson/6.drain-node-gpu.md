## 노드 배제 ##

아래 내용은 slurm 에서 특정 노드를 배제하는 방법에 대한 내용이다. slinky 에서는 어떻게 설정하는지는 확인이 필요하다. 

#### 1. 사용자: 특정 노드 제외하고 제출 (--exclude) ####
작업 제출 시 --exclude (또는 -x) 옵션을 사용하면 특정 노드를 스케줄링에서 완전히 뺄 수 있습니다.
```
# gpu-node05 노드만 쏙 빼고 작업 제출
sbatch --exclude=gpu-node05 submit.sh
srun --exclude=gpu-node01,gpu-node02 --gres=gpu:1 python train.py
```

#### 2. 관리자: 노드 상태를 'Drain'으로 변경 ####
노드에 문제가 있어 아무도 쓰지 못하게 격리해야 할 때 사용합니다. 기존 작업은 유지되거나 종료될 때까지 기다리지만, 새 작업은 절대 할당되지 않습니다.
```
# 특정 노드를 예약 리스트에서 제외
sudo scontrol update NodeName=gpu-node05 State=DRAIN Reason="GPU hardware issue"
```
복구시는 State=RESUME 으로 복구한다.

#### 3. 영구 제외 (slurm.conf 수정) ####
* /etc/slurm/slurm.conf 파일에서 해당 NodeName=... 줄을 주석 처리(법#)하거나 삭제합니다.
* Partition 설정에서도 해당 노드 이름을 제거합니다.
* sudo scontrol reconfigure 명령어로 설정을 갱신합니다


## GPU 배제 ##
Slurm 관리자 수준에서 특정 GPU를 제외하는 두 가지 구체적인 방법은 다음과 같다. 이 역시 slniky 에서 어떻게 하는지는 확인이 필요하다

#### 1. GRES 설정을 통한 논리적 제외 (추천) ####
Slurm이 인식하는 GPU 리소스 목록에서 특정 장치를 삭제하는 방법으로, 이 설정은 Slurm이 해당 GPU를 사용자에게 할당하지 않도록 만든다. 
#### gres.conf 수정 ####
```
# /etc/slurm/gres.conf 수정
# AutoDetect=nvml  <-- 자동 인식을 끄거나 특정 노드에 대해 수동 설정을 우선.
NodeName=gpu-node01 Name=gpu Type=nvidia File=/dev/nvidia[0-1]
NodeName=gpu-node01 Name=gpu Type=nvidia File=/dev/nvidia[3-7]
```
2번 GPU(/dev/nvidia2)를 제외하였다.

#### slurm.conf 확인 #### 
slurm 은 쿠버네티스와 마찬가지로 CPU와 메모리만 관리한다. GPU 같은 추가 자원을 관리하려면 Generic Resource(GRES) 사용 설정을 명시하여야 한다.
slurm.conf 에 GresTypes=gpu 라인이 포함되어 있는지 확인해야 한다. 

#### 서비스 재시작 #### 
변경 사항을 적용하려면 노드와 마스터에서 데몬을 재시작해야 한다.
```
# 계산 노드에서
sudo systemctl restart slurmd
# 마스터 노드에서 (또는 scontrol reconfigure)
sudo scontrol reconfigure
```

#### 2. nvidia-smi를 통한 물리적 제외 (Drain) #### 
Slurm 설정과 별개로, GPU 자체를 'Prohibited(금지)' 모드로 설정하여 어떤 프로세스도 해당 GPU에서 실행되지 못하게 막는 방법이다.
```
# 2번 GPU를 '작업 금지' 상태로 변경 (루트 권한 필요)
sudo nvidia-smi -i 2 -c PROHIBITED
```
* 이 설정은 즉시 반영되지만, 재부팅 시 초기화될 수 있으므로 영구 적용을 원하면 nvidia-persistenced 설정을 병행해야 한다.
* Slurm 스케줄러는 하드웨어의 내부 상태(Compute Mode)를 실시간으로 감시하지 않는다. GPU 를 배제하다라도 slurm 설정을 변경해 줘야 한다. 그렇지 않으면 작동하지 않는 GPU에 사용자 작업에 할당해 버린다. 

